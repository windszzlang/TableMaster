{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/rm/ssd2/langcao/workspace/tablemaster\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.chdir('/home/rm/ssd2/langcao/workspace/tablemaster')\n",
    "!pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import json\n",
    "import glob\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "sys.path.append('./')\n",
    "from evaluate.evaluator import eval_qa, eval_fact"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Count:  4344\n",
      "True:  1373\n",
      "False:  2971\n",
      "Percentage:  0.31606813996316757\n"
     ]
    }
   ],
   "source": [
    "dataset = 'wikitq'\n",
    "# dataset = 'tabfact'\n",
    "\n",
    "# files = glob.glob(f'outputs/analysis/number/{dataset}/*.json')\n",
    "files = glob.glob(f'outputs/analysis/number-4m-self/{dataset}/*.json')\n",
    "files = sorted(files, key=lambda x: int(x.split('/')[-1].split('.')[0]))\n",
    "need_calculation = []\n",
    "for file in files:\n",
    "    with open(file, 'r') as f:\n",
    "        D = json.load(f)\n",
    "        need_calculation.append(D['need_calculation'])\n",
    "\n",
    "# statistics\n",
    "print('Count: ', len(need_calculation))\n",
    "print('True: ', sum(need_calculation))\n",
    "print('False: ', len(need_calculation) - sum(need_calculation))\n",
    "print('Percentage: ', sum(need_calculation) / len(need_calculation))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reasoning Method Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "models = ['gpt35', 'gpt4m', 'gpt4o']\n",
    "# models = ['gpt4m']\n",
    "# models = ['gpt35']\n",
    "\n",
    "methods = ['pot', 'cot', 'guided_pot']\n",
    "# methods = ['direct', 'pot', 'cot', 'guided_pot']\n",
    "\n",
    "\n",
    "for model in models:\n",
    "\n",
    "    for method in methods:\n",
    "        # if method == 'guided_pot' and model != 'gpt35':\n",
    "            # continue\n",
    "        print(f'Model: {model} - Method: {method}')\n",
    "        pred_path = f'outputs/baselines/{dataset}/{model}/{method}'\n",
    "\n",
    "        pred_files = glob.glob(f'{pred_path}/*.json')\n",
    "        pred_files = sorted(pred_files, key=lambda x: int(x.split('/')[-1].split('.')[0]))\n",
    "\n",
    "        results = []\n",
    "        results_of_need_calculation = []\n",
    "        results_of_not_need_calculation = []\n",
    "\n",
    "        for i, file in enumerate(pred_files):\n",
    "            with open(file, 'r') as f:\n",
    "                D = json.load(f)\n",
    "                if dataset == 'wikitq':\n",
    "                    result = eval_qa(D['predicted_answer'], D['answer'])\n",
    "                elif dataset == 'tabfact':\n",
    "                    result = eval_fact(D['predicted_answer'], D['answer'])\n",
    "                results.append(result)\n",
    "\n",
    "                if need_calculation[i]:\n",
    "                    results_of_need_calculation.append(result)\n",
    "                else:\n",
    "                    results_of_not_need_calculation.append(result)\n",
    "\n",
    "        results = np.array(results).mean().tolist()\n",
    "        results_of_need_calculation = np.array(results_of_need_calculation).mean().tolist()\n",
    "        results_of_not_need_calculation = np.array(results_of_not_need_calculation).mean().tolist()\n",
    "\n",
    "        print(f'Overall: {results}')\n",
    "        print(f'Need Calculation: {results_of_need_calculation}')\n",
    "        print(f'Not Need Calculation: {results_of_not_need_calculation}\\n')\n",
    "\n",
    "    print('-' * 20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Adaptive Reasoning Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Count:  4344\n",
      "True:  1590\n",
      "False:  2754\n",
      "Percentage:  0.3660220994475138 \n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CoT: 72.97\n",
      "POT: 68.83\n",
      "Combined: 74.08\n",
      "\n",
      "Best: 85.06\n",
      "Best POT Count: 525\n"
     ]
    }
   ],
   "source": [
    "# adaptive with o1\n",
    "# model = 'gpt35'\n",
    "model = 'gpt4m'\n",
    "# model = 'gpt4o'\n",
    "\n",
    "dataset = 'wikitq'\n",
    "# dataset = 'tabfact'\n",
    "\n",
    "\n",
    "files = glob.glob(f'outputs/analysis/number-4m-self/{dataset}/*.json')\n",
    "# files = glob.glob(f'outputs/analysis/number/{dataset}/*.json')\n",
    "files = sorted(files, key=lambda x: int(x.split('/')[-1].split('.')[0]))\n",
    "need_calculation = []\n",
    "for file in files:\n",
    "    with open(file, 'r') as f:\n",
    "        D = json.load(f)\n",
    "        need_calculation.append(D['need_calculation'])\n",
    "\n",
    "\n",
    "# statistics\n",
    "print('Count: ', len(need_calculation))\n",
    "print('True: ', sum(need_calculation))\n",
    "print('False: ', len(need_calculation) - sum(need_calculation))\n",
    "print('Percentage: ', sum(need_calculation) / len(need_calculation), '\\n')\n",
    "\n",
    "\n",
    "cot_pred_path = f'outputs/baselines/{dataset}/{model}/cot'\n",
    "# pot_pred_path = f'outputs/baselines/{dataset}/{model}/pot'\n",
    "pot_pred_path = f'outputs/baselines/{dataset}/{model}/guided_pot'\n",
    "\n",
    "cot_pred_files = glob.glob(f'{cot_pred_path}/*.json')\n",
    "cot_pred_files = sorted(cot_pred_files, key=lambda x: int(x.split('/')[-1].split('.')[0]))\n",
    "\n",
    "pot_pred_files = glob.glob(f'{pot_pred_path}/*.json')\n",
    "pot_pred_files = sorted(pot_pred_files, key=lambda x: int(x.split('/')[-1].split('.')[0]))\n",
    "\n",
    "\n",
    "cot_results = []\n",
    "pot_results = []\n",
    "combined_results = []\n",
    "best_results = []\n",
    "best_results_pot_count = 0\n",
    "\n",
    "for i, (cot_file, pot_file) in enumerate(zip(cot_pred_files, pot_pred_files)):\n",
    "\n",
    "\n",
    "    with open(cot_file, 'r') as f:\n",
    "        cot_D = json.load(f)\n",
    "        if dataset == 'wikitq':\n",
    "            cot_result = eval_qa(cot_D['predicted_answer'], cot_D['answer'])\n",
    "        elif dataset == 'tabfact':\n",
    "            cot_result = eval_fact(cot_D['predicted_answer'], cot_D['answer'])\n",
    "        cot_results.append(cot_result)\n",
    "\n",
    "    with open(pot_file, 'r') as f:\n",
    "        pot_D = json.load(f)\n",
    "        if dataset == 'wikitq':\n",
    "            pot_result = eval_qa(pot_D['predicted_answer'], pot_D['answer'])\n",
    "        elif dataset == 'tabfact':\n",
    "            pot_result = eval_fact(pot_D['predicted_answer'], pot_D['answer'])\n",
    "        best_results.append(max(cot_result, pot_result))\n",
    "\n",
    "        if pot_result and not cot_result:\n",
    "            best_results_pot_count += 1\n",
    "        \n",
    "        cot_results.append(cot_result)\n",
    "        pot_results.append(pot_result)\n",
    "\n",
    "    if i >= len(need_calculation):\n",
    "        continue\n",
    "    if need_calculation[i]:\n",
    "        combined_results.append(pot_result)\n",
    "    else:\n",
    "        combined_results.append(cot_result)\n",
    "    \n",
    "\n",
    "cot_results = np.array(cot_results).mean().tolist() * 100\n",
    "pot_results = np.array(pot_results).mean().tolist() * 100\n",
    "combined_results = np.array(combined_results).mean().tolist() * 100\n",
    "best_results = np.array(best_results).mean().tolist() * 100\n",
    "\n",
    "print(f'CoT: {cot_results:.2f}')\n",
    "print(f'POT: {pot_results:.2f}')\n",
    "print(f'Combined: {combined_results:.2f}\\n')\n",
    "print(f'Best: {best_results:.2f}')\n",
    "print(f'Best POT Count: {best_results_pot_count}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TableMaster Adapative Reasoning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TableMaster adaptive reasoning performance\n",
      "CoT: 72.78, number of questions: 2520\n",
      "POT: 85.53, number of questions: 1824\n",
      "TableMaster performance in two types of questions\n",
      "No need calculation: 83.90, number of questions: 1652\n",
      "Need calculation: 74.59, number of questions: 2692\n"
     ]
    }
   ],
   "source": [
    "# adaptive with o1\n",
    "model = 'tablemaster-4m'\n",
    "# model = 'tablemaster-4o'\n",
    "# model = 'tablemaster-35'\n",
    "\n",
    "dataset = 'wikitq'\n",
    "# dataset = 'tabfact'\n",
    "\n",
    "\n",
    "pred_path = f'outputs/main/{dataset}/{model}'\n",
    "\n",
    "pred_files = glob.glob(f'{pred_path}/*.json')\n",
    "pred_files = sorted(pred_files, key=lambda x: int(x.split('/')[-1].split('.')[0]))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "results = []\n",
    "self_strategy = []\n",
    "o1_strategy = need_calculation\n",
    "\n",
    "for i, pred_file in enumerate(pred_files):\n",
    "\n",
    "\n",
    "    with open(pred_file, 'r') as f:\n",
    "        pred_D = json.load(f)\n",
    "        if dataset == 'wikitq':\n",
    "            result = eval_qa(pred_D['predicted_answer'], pred_D['answer'])\n",
    "        elif dataset == 'tabfact':\n",
    "            result = eval_fact(pred_D['predicted_answer'], pred_D['answer'])\n",
    "        \n",
    "        results.append(result)\n",
    "\n",
    "        if pred_D['reasoning_process']['symbolic_reasoning_process'] == '':\n",
    "            self_strategy.append(1)\n",
    "        else:\n",
    "            self_strategy.append(0)\n",
    "\n",
    "\n",
    "\n",
    "print('TableMaster adaptive reasoning performance')\n",
    "strategy = self_strategy\n",
    "cot_results = []\n",
    "pot_results = []\n",
    "\n",
    "for i, result in enumerate(results):\n",
    "    if strategy[i]:\n",
    "        pot_results.append(result)\n",
    "    else:\n",
    "        cot_results.append(result)    \n",
    "\n",
    "mean_cot_results = np.array(cot_results).mean().tolist() * 100\n",
    "mean_pot_results = np.array(pot_results).mean().tolist() * 100\n",
    "\n",
    "print(f'CoT: {mean_cot_results:.2f}, number of questions: {len(cot_results)}')\n",
    "print(f'POT: {mean_pot_results:.2f}, number of questions: {len(pot_results)}')\n",
    "\n",
    "print('TableMaster performance in two types of questions')\n",
    "strategy = o1_strategy\n",
    "cot_results = []\n",
    "pot_results = []\n",
    "\n",
    "for i, result in enumerate(results):\n",
    "    if strategy[i]:\n",
    "        pot_results.append(result)\n",
    "    else:\n",
    "        cot_results.append(result)    \n",
    "\n",
    "mean_cot_results = np.array(cot_results).mean().tolist() * 100\n",
    "mean_pot_results = np.array(pot_results).mean().tolist() * 100\n",
    "\n",
    "print(f'No need calculation: {mean_cot_results:.2f}, number of questions: {len(cot_results)}')\n",
    "print(f'Need calculation: {mean_pot_results:.2f}, number of questions: {len(pot_results)}')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "langcao",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
